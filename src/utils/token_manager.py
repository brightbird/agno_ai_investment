"""
Tokenç®¡ç†å·¥å…·
å¤„ç†AIæ¨¡å‹çš„tokené™åˆ¶é—®é¢˜
"""

import re
import json
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass


@dataclass
class TokenBudget:
    """Tokené¢„ç®—é…ç½®"""
    max_total_tokens: int = 8000  # æ€»tokené™åˆ¶
    max_input_tokens: int = 6000  # è¾“å…¥tokené™åˆ¶
    max_output_tokens: int = 2000  # è¾“å‡ºtokené™åˆ¶
    reserve_tokens: int = 500     # é¢„ç•™token
    

class TokenManager:
    """Tokenç®¡ç†å™¨"""
    
    def __init__(self, budget: TokenBudget = None):
        self.budget = budget or TokenBudget()
        
    def estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
        ç®€å•ä¼°ç®—ï¼šä¸­æ–‡æŒ‰å­—æ•°ï¼Œè‹±æ–‡æŒ‰å•è¯æ•°*1.3
        """
        if not text:
            return 0
            
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        
        # ç»Ÿè®¡è‹±æ–‡å•è¯
        english_words = len(re.findall(r'[a-zA-Z]+', text))
        
        # ç»Ÿè®¡æ•°å­—å’Œç¬¦å·
        other_chars = len(re.sub(r'[\u4e00-\u9fff\sa-zA-Z]', '', text))
        
        # ä¼°ç®—tokenæ•°
        estimated_tokens = chinese_chars + int(english_words * 1.3) + int(other_chars * 0.5)
        
        return max(estimated_tokens, 1)
    
    def truncate_text(self, text: str, max_tokens: int) -> str:
        """
        æˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®štokenæ•°
        """
        current_tokens = self.estimate_tokens(text)
        
        if current_tokens <= max_tokens:
            return text
            
        # è®¡ç®—æˆªæ–­æ¯”ä¾‹
        ratio = max_tokens / current_tokens * 0.9  # ç•™10%å®‰å…¨è¾¹é™…
        target_length = int(len(text) * ratio)
        
        # ä»ä¸­é—´æˆªæ–­ï¼Œä¿ç•™å¼€å¤´å’Œç»“å°¾
        if target_length < len(text):
            keep_start = target_length // 2
            keep_end = target_length - keep_start
            truncated = text[:keep_start] + "\n\n[... å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­ ...]\n\n" + text[-keep_end:]
            return truncated
        
        return text
    
    def compress_analysis_results(self, analyses: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å‹ç¼©åˆ†æç»“æœï¼Œæå–å…³é”®ä¿¡æ¯
        """
        compressed_analyses = []
        
        for analysis in analyses:
            # æå–å…³é”®ä¿¡æ¯
            compressed = {
                "agent": analysis.get("agent", ""),
                "symbol": analysis.get("symbol", ""),
                "style": analysis.get("style", ""),
                "summary": self._extract_summary(analysis.get("analysis", "")),
                "recommendation": self._extract_recommendation(analysis.get("analysis", "")),
                "key_points": self._extract_key_points(analysis.get("analysis", ""))
            }
            compressed_analyses.append(compressed)
        
        return compressed_analyses
    
    def _extract_summary(self, analysis_text: str) -> str:
        """æå–åˆ†ææ‘˜è¦"""
        if not analysis_text:
            return ""
            
        # å¯»æ‰¾æ‘˜è¦ç›¸å…³çš„æ®µè½
        summary_patterns = [
            r"##?\s*æ‘˜è¦[\s\S]*?(?=##|\n\n|\Z)",
            r"##?\s*æ€»ç»“[\s\S]*?(?=##|\n\n|\Z)",
            r"##?\s*ç»“è®º[\s\S]*?(?=##|\n\n|\Z)",
            r"##?\s*æŠ•èµ„å»ºè®®[\s\S]*?(?=##|\n\n|\Z)"
        ]
        
        for pattern in summary_patterns:
            match = re.search(pattern, analysis_text, re.IGNORECASE)
            if match:
                summary = match.group(0)
                return self.truncate_text(summary, 200)
        
        # å¦‚æœæ²¡æ‰¾åˆ°ä¸“é—¨çš„æ‘˜è¦ï¼Œå–å‰200ä¸ªtoken
        return self.truncate_text(analysis_text, 200)
    
    def _extract_recommendation(self, analysis_text: str) -> str:
        """æå–æŠ•èµ„å»ºè®®"""
        if not analysis_text:
            return "æœªæ˜ç¡®"
            
        # å¯»æ‰¾æ¨èç›¸å…³çš„è¯æ±‡
        buy_patterns = [
            r"å¼ºçƒˆæ¨è|å¼ºçƒˆä¹°å…¥|ä¹°å…¥|æ¨èè´­ä¹°",
            r"å»ºè®®ä¹°å…¥|å»ºè®®è´­ä¹°|å€¼å¾—æŠ•èµ„",
            r"BUY|Strong Buy|Outperform"
        ]
        
        hold_patterns = [
            r"æŒæœ‰|ç»§ç»­æŒæœ‰|ç»´æŒ",
            r"HOLD|Neutral"
        ]
        
        sell_patterns = [
            r"å–å‡º|å‡æŒ|å»ºè®®å–å‡º",
            r"SELL|Underperform"
        ]
        
        text_lower = analysis_text.lower()
        
        for pattern in buy_patterns:
            if re.search(pattern, analysis_text, re.IGNORECASE):
                return "ä¹°å…¥"
        
        for pattern in sell_patterns:
            if re.search(pattern, analysis_text, re.IGNORECASE):
                return "å–å‡º"
                
        for pattern in hold_patterns:
            if re.search(pattern, analysis_text, re.IGNORECASE):
                return "æŒæœ‰"
        
        return "æœªæ˜ç¡®"
    
    def _extract_key_points(self, analysis_text: str) -> List[str]:
        """æå–å…³é”®è¦ç‚¹"""
        if not analysis_text:
            return []
            
        key_points = []
        
        # å¯»æ‰¾åˆ—è¡¨é¡¹æˆ–è¦ç‚¹
        bullet_patterns = [
            r"[-*â€¢]\s*([^\n]+)",
            r"\d+[.)]\s*([^\n]+)",
            r"[â–ªâ–«]\s*([^\n]+)"
        ]
        
        for pattern in bullet_patterns:
            matches = re.findall(pattern, analysis_text)
            for match in matches:
                if len(match.strip()) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                    key_points.append(match.strip())
        
        # é™åˆ¶è¦ç‚¹æ•°é‡å’Œé•¿åº¦
        key_points = key_points[:5]  # æœ€å¤š5ä¸ªè¦ç‚¹
        key_points = [self.truncate_text(point, 50) for point in key_points]
        
        return key_points
    
    def split_large_analysis(self, prompt: str, max_tokens: int) -> List[str]:
        """
        å°†å¤§çš„åˆ†æè¯·æ±‚åˆ†å‰²ä¸ºå¤šä¸ªå°è¯·æ±‚
        """
        current_tokens = self.estimate_tokens(prompt)
        
        if current_tokens <= max_tokens:
            return [prompt]
        
        # å°è¯•æŒ‰æ®µè½åˆ†å‰²
        paragraphs = prompt.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            test_chunk = current_chunk + "\n\n" + paragraph if current_chunk else paragraph
            
            if self.estimate_tokens(test_chunk) <= max_tokens:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = paragraph
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def create_batched_analysis(self, symbols: List[str], batch_size: int = 3) -> List[List[str]]:
        """
        å°†è‚¡ç¥¨åˆ—è¡¨åˆ†æ‰¹å¤„ç†
        """
        batches = []
        for i in range(0, len(symbols), batch_size):
            batch = symbols[i:i + batch_size]
            batches.append(batch)
        return batches
    
    def optimize_prompt_template(self, template: str) -> str:
        """
        ä¼˜åŒ–promptæ¨¡æ¿ï¼Œå‡å°‘tokenä½¿ç”¨
        """
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        optimized = re.sub(r'\n{3,}', '\n\n', template)
        
        # å‹ç¼©é‡å¤çš„åˆ†éš”ç¬¦
        optimized = re.sub(r'={3,}', '===', optimized)
        optimized = re.sub(r'-{3,}', '---', optimized)
        
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
        optimized = re.sub(r' {2,}', ' ', optimized)
        
        # ç®€åŒ–è¡¨æ ¼æ ¼å¼æŒ‡ä»¤
        table_simplifications = {
            r'è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡º': 'æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡º',
            r'è¯·åŸºäºä»¥ä¸‹.*?çš„åˆ†æ': 'åŸºäºä»¥ä¸‹åˆ†æ',
            r'ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ç»¼åˆæŠ•èµ„æŠ¥å‘Š': 'ç”Ÿæˆç»¼åˆæŠ•èµ„æŠ¥å‘Š'
        }
        
        for pattern, replacement in table_simplifications.items():
            optimized = re.sub(pattern, replacement, optimized)
        
        return optimized.strip()


class StreamingAnalyzer:
    """æµå¼åˆ†æå™¨ï¼Œæ”¯æŒå¤§å†…å®¹çš„åˆ†æ®µå¤„ç†"""
    
    def __init__(self, token_manager: TokenManager):
        self.token_manager = token_manager
        
    def stream_multi_master_analysis(self, symbol: str, analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æµå¼å¤„ç†å¤šæŠ•èµ„å¤§å¸ˆåˆ†æ
        """
        # å‹ç¼©åˆ†æç»“æœ
        compressed_analyses = self.token_manager.compress_analysis_results(analyses)
        
        # ç”Ÿæˆåˆ†æ®µæŠ¥å‘Š
        sections = {
            "executive_summary": self._generate_executive_summary(symbol, compressed_analyses),
            "master_opinions": self._generate_master_opinions(compressed_analyses),
            "risk_assessment": self._generate_risk_assessment(symbol, compressed_analyses),
            "investment_plan": self._generate_investment_plan(symbol, compressed_analyses)
        }
        
        return {
            "symbol": symbol,
            "sections": sections,
            "compressed_analyses": compressed_analyses
        }
    
    def _generate_executive_summary(self, symbol: str, analyses: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        prompt = f"""
åŸºäºä»¥ä¸‹{len(analyses)}ä½æŠ•èµ„å¤§å¸ˆå¯¹{symbol}çš„åˆ†ææ‘˜è¦ï¼Œç”Ÿæˆç®€æ´çš„æ‰§è¡Œæ‘˜è¦ï¼š

{json.dumps(analyses, ensure_ascii=False, indent=2)}

è¯·è¾“å‡ºï¼š
## ğŸ¯ æ‰§è¡Œæ‘˜è¦

| é¡¹ç›® | ç»“è®º |
|------|------|
| æœ€ç»ˆå»ºè®® | [ä¹°å…¥/æŒæœ‰/å–å‡º] |
| ç»¼åˆè¯„åˆ† | [X/10åˆ†] |
| é£é™©ç­‰çº§ | [ä½/ä¸­/é«˜] |

## ğŸ“Š ä¸€è‡´æ€§è§‚ç‚¹
- [3å¥è¯æ€»ç»“å¤§å¸ˆä»¬çš„ä¸€è‡´è§‚ç‚¹]

## âš ï¸ ä¸»è¦é£é™©
- [æœ€é‡è¦çš„2-3ä¸ªé£é™©ç‚¹]
"""
        return self.token_manager.truncate_text(prompt, 1000)
    
    def _generate_master_opinions(self, analyses: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆå¤§å¸ˆè§‚ç‚¹å¯¹æ¯”"""
        prompt = f"""
åŸºäºä»¥ä¸‹æŠ•èµ„å¤§å¸ˆè§‚ç‚¹æ‘˜è¦ï¼Œç”Ÿæˆè§‚ç‚¹å¯¹æ¯”ï¼š

{json.dumps(analyses, ensure_ascii=False, indent=2)}

è¯·è¾“å‡ºï¼š
## ğŸ­ æŠ•èµ„å¤§å¸ˆè§‚ç‚¹å¯¹æ¯”

| å¤§å¸ˆ | å»ºè®® | æ ¸å¿ƒé€»è¾‘ | ä¿¡å¿ƒåº¦ |
|------|------|----------|--------|
{self._create_master_table_rows(analyses)}

## è§‚ç‚¹åˆ†æ
- ä¸€è‡´è§‚ç‚¹ï¼š[æ€»ç»“]
- ä¸»è¦åˆ†æ­§ï¼š[æ€»ç»“]
"""
        return self.token_manager.truncate_text(prompt, 1000)
    
    def _generate_risk_assessment(self, symbol: str, analyses: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆé£é™©è¯„ä¼°"""
        prompt = f"""
åŸºäºæŠ•èµ„å¤§å¸ˆåˆ†æï¼Œä¸º{symbol}ç”Ÿæˆé£é™©è¯„ä¼°ï¼š

è¯·è¾“å‡ºï¼š
## âš ï¸ é£é™©è¯„ä¼°

| é£é™©ç±»å‹ | ç¨‹åº¦ | åº”å¯¹ç­–ç•¥ |
|----------|------|----------|
| ä¼°å€¼é£é™© | [é«˜/ä¸­/ä½] | [ç­–ç•¥] |
| ä¸šç»©é£é™© | [é«˜/ä¸­/ä½] | [ç­–ç•¥] |
| å¸‚åœºé£é™© | [é«˜/ä¸­/ä½] | [ç­–ç•¥] |

## é£é™©æ§åˆ¶
- æ­¢æŸä½ï¼š[ä»·æ ¼]
- ç›‘æ§æŒ‡æ ‡ï¼š[å…³é”®æŒ‡æ ‡]
"""
        return self.token_manager.truncate_text(prompt, 800)
    
    def _generate_investment_plan(self, symbol: str, analyses: List[Dict[str, Any]]) -> str:
        """ç”ŸæˆæŠ•èµ„è®¡åˆ’"""
        prompt = f"""
ä¸º{symbol}ç”ŸæˆæŠ•èµ„æ‰§è¡Œè®¡åˆ’ï¼š

## ğŸ¯ æŠ•èµ„æ‰§è¡Œè®¡åˆ’

### ä¹°å…¥ç­–ç•¥
| æŠ•èµ„è€…ç±»å‹ | å»ºè®®ä»“ä½ | ä¹°å…¥ä»·ä½ |
|------------|----------|----------|
| ä¿å®ˆå‹ | [X]% | $[ä»·æ ¼] |
| å¹³è¡¡å‹ | [X]% | $[ä»·æ ¼] |
| æ¿€è¿›å‹ | [X]% | $[ä»·æ ¼] |

### æ—¶é—´è§„åˆ’
- çŸ­æœŸç›®æ ‡ï¼š[1-6ä¸ªæœˆ]
- ä¸­æœŸç›®æ ‡ï¼š[6-18ä¸ªæœˆ]
- é•¿æœŸç›®æ ‡ï¼š[18ä¸ªæœˆ+]
"""
        return self.token_manager.truncate_text(prompt, 600)
    
    def _create_master_table_rows(self, analyses: List[Dict[str, Any]]) -> str:
        """åˆ›å»ºå¤§å¸ˆè§‚ç‚¹è¡¨æ ¼è¡Œ"""
        rows = []
        for analysis in analyses:
            master = analysis.get("agent", "").split("ä»·å€¼æŠ•èµ„åˆ†æå¸ˆ")[0].strip()
            recommendation = analysis.get("recommendation", "æœªæ˜ç¡®")
            summary = analysis.get("summary", "")[:50]  # æˆªæ–­åˆ°50å­—ç¬¦
            rows.append(f"| {master} | {recommendation} | {summary} | ä¸­ç­‰ |")
        return "\n".join(rows)


def create_token_optimized_agents():
    """åˆ›å»ºtokenä¼˜åŒ–çš„agenté…ç½®"""
    return {
        "model_config": {
            "max_tokens": 1500,  # å‡å°‘å•æ¬¡è¾“å‡ºtoken
            "temperature": 0.7,
            "stream": True  # å¯ç”¨æµå¼è¾“å‡º
        },
        "prompt_optimization": {
            "use_compression": True,
            "batch_processing": True,
            "summary_mode": True
        }
    } 